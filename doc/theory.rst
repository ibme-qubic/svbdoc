Stochastic Variational Bayes - Theory
=====================================

Stochastic Variational Bayes is a method of performing Bayesian inference on the parameters
of a generative model given a data set assumed to be generated by the model with 
unknown additive noise.

The description below is a highly abbreviated account of the theory behind SVB.
For more detailed derivations, see the references cited.

Bayesian inference
------------------

Bayes' theorem is a general statement about how ones belief about the value distribution
of a random variable should be updated in the light of new data. In the context of
model parameter inference it can be described as follows:

.. math::

    p(\theta \mid y) = q(\theta) = \frac{p(y \mid \theta) \, p(\theta)}{p(y)}

Here :math:`\theta` is the set of parameters of the model which we wish to infer.

:math:`p(\theta \mid y) = q(\theta)` is the *posterior* distribution, i.e. the inferred 
distribution of the model parameters :math:`\theta` given the data :math:`y`.

:math:`p(\theta)` is the *prior* probability of the model parameters :math:`\theta`. This describes the
distribution we believe the parameters would follow before any data has been seen
and might reflect, for example, existing estimates of physiological parameters or other
constraints (e.g. that a fractional parameter must lie between 0 and 1).

:math:`p(y \mid \theta)` is the *likelihood*, i.e. the probability of getting the data :math:`y`
from a given set of model parameters :math:`\theta`. This is determined by evaluating the model
prediction using the parameters :math:`\theta` and comparing it to the data. The difference between
the two must be the result of noise, and the likelihood of the noise can be calculated
from the noise model.

:math:`p(y)` is the *evidence* and is chiefly used when comparing one model with another.
For an inference problem using a single model it can be neglected as it is independent
of the parameters and simply provides a normalizing constant.

Variational Bayes
-----------------

The general Bayesian inference problem can, in general, only be solved by a sampling
method such as Markov Chain Monte Carlo (MCMC) where random samples are generated in
such a way that, through Bayes' theorem, they gradually provide a representative 
sample of the posterior distribution. Any properties of the posterior, such as mean
and variance, can be calculated from the sample once it is large enough to be
representative.

MCMC, however, is extremely computationally intensive especially for the kind of 
applications we are concerned with where we may be fitting between 2 and 20 parameters
independently at typically :math:`10^5` voxels. Variational Bayes is an approximate
method which re-formulates the inference problem in the form of a variational
principle, where we seek to maximise the *Free Energy*.

.. math::

    F(\theta) = \int q(\theta)\log \bigg( p(y \mid \theta)\frac{p(\theta)}{q(\theta)} \bigg) d\theta
    
Again :math:`\theta` is the set of model parameters, :math:`q(\theta)` is the posterior
distribution, :math:`p(\theta)` is the prior distribution and :math:`p(y \mid \theta)`
is the likelihood of the data given the parameters.

For completely general forms of the prior and posterior distributions, this integral
is expensive to compute numerically (and is unlikely to be solvable analytically).
However the advantage of the variational approach is that simplified forms can be chosen for the
prior and posterior such that the free energy can be calculated and optimized 
efficiently. The variational principle guarantees that the free energy calculated
using this method will be a lower bound on the 'true' free energy and therefore the
closest approximation we can find using our simplified distributions.

Typically we assume multivariate Gaussian 
distributions for the prior and posterior, and a noise model based on a Gaussian or
Gamma distribution.

One form of variational Bayes uses the calculus of variations to derive a set of
update equations for the model and noise parameters which can then be iterated 
until convergence. However this method requires particular choices of the prior
and posterior distributions, and the noise model, and thus lacks flexibility.
Any change to these distributions requires the update equations to be 
re-derived.

Stochastic variational Bayes
----------------------------

The free energy equation can be slightly re-written in the form of an expectation over the
posterior distribution :math:`q(\theta)`:

.. math::

    F(\theta) = E_{q(\theta)} \big[ \log(p(y \mid \theta) \big] - E_{q(\theta)} \bigg[ \log \Big( \frac{q(\theta)}{p(\theta)} \Big) \bigg]

This suggests an alternative calculation method based on taking a *sample* of
values from the posterior distribution. If this sample is large enough to be 
representative of the distribution, the expectation integrals from above can be approximated
by the mean over the samples:

.. math::

    E_{q(\theta)} \big[ \log(p(y \mid \theta) \big] \approx \frac{1}{S} \sum_s \log(p(y \mid \theta^s)

    E_{q(\theta)} \bigg[ \log \Big( \frac{q(\theta)}{p(\theta)} \Big) \bigg] \approx \frac{1}{S} \sum_s \bigg[ \log \Big( \frac{q(\theta^s)}{p(\theta^s)} \Big) \bigg]

Where we have :math:`S` samples of the full set of parameters, denoted :math:`\theta^s`.

The first of these terms is the negative of the *reconstruction loss* and is a measure of
how well the model prediction fits the data.

The second term is the *latent loss* and measures the closeness of the posterior
to the prior. In fact it is the Kullback-Leibler (KL) divergence between the
prior and posterior distributions.

This is more tractable than a numerical integration *provided* we can obtain
a representative sample from the posterior. Maximisation of the free energy
can then be done using a generic framework such as those developed for machine
learning applications which have the ability to automatically calculate gradients
of an objective function from a defined set of calculation steps.

Alternative forms of the latent loss
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The latent loss term can be alternatively written as follows, removing the stochastic approximation
for part of the log:

.. math::

    E_{q(\theta)} \bigg[ \log \Big( \frac{q(\theta)}{p(\theta)} \Big) \bigg] \approx E_{q(\theta)} \bigg[ \log(q(\theta)) \bigg] - \frac{1}{S} \sum_s \bigg[ \log ( p(\theta^s) ) \bigg]

The first term is the *entropy* of the posterior distribution. For many distributions this 
can be calculated analytically without reference to a sample, so we may be able reduce our
dependence on the choice of sample to some degree.

If both the prior and posterior are multivariate Gaussian distributions, we can 
go further and obtain a fully analytic expression for the latent loss using the known
result for the KL divergence of two MVNs:

.. math::

    E_{q(\theta)} \bigg[ \log \Big( \frac{q(\theta)}{p(\theta)} \Big) \bigg] = \frac{1}{2} \bigg\{ \mathrm{Tr}(\Sigma_p^{-1} \Sigma_q) + (\mu_p - \mu_q)^T\Sigma_p^{-1}(\mu_p - \mu_q) - N + \log\bigg( \frac{\det \Sigma_p}{\det \Sigma_q} \bigg)  \bigg\}

Here :math:`N` is the number of parameters in :math:`\theta`, and 
:math:`\mu_p, \Sigma_p, \mu_q, \Sigma_q` are the mean and covariance of the 
prior and posterior.

Obtaining the sample from the posterior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The problem of sampling from the posterior is of some significance. If the 
optimization is to work effectively it would be helpful if the gradients
of the sample values with respect to the variable parameters could be 
calculated. However this is difficult if we simply obtain a random 
sample from, for example, a Gaussian of given mean and variance. For 
Gaussian distributions, one way around this is known as the *reparameterization 
trick*. We obtain a sample from a *fixed* Gaussian (e.g. :math:`N(0, 1)`) and
then scale the values using the (variable) mean and variance of the posterior
distribution. This enables the gradients to be used in the optimization 
algorithm. The disadvantage of the method is that it does not immediately
generalise to other kinds of distributions.

Advantages of the stochastic approach
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The main advantage of the stochastic approach is that the requirements on
the prior and posterior distributions are greatly reduced. The prior
distribution needs to be able to generate log probabilities for a set of
parameters, the posterior needs to be able to generate samples and its
own entropy, and we need some means of calculating the data likelihood
- this normally involves a noise model which can calculate the
probability of the observed deviations between a model prediction
and the actual data. Although we can take advantage of analytic results for
Gaussian distribution, the actual forms of the distributions are not 
constrained by the method (apart from the limitation of not always being able to use
the reparameterization trick).

Spatial regularization priors
-----------------------------

When inferring model parameters from noisy data, it is to be expected that the output
parameter maps are themselves rather noisy. However in many cases we know that 
parameter values are unlikely to vary dramatically between voxels whose spatial 
positions are very similar. For this reason it is common to perform some kind of
spatial smoothing, either on the initial input data, or on the output parameter maps.
Usually this consists of *ad-hoc* Gaussian smoothing using a kernel determined from
inspection of the data and/or output.

The Bayesian framework offers us an alternative means of achieving the same effect in
a more principled way which responds directly to the information present in the data.
In essence we embody our belief in a degree of spatial uniformity into the *prior*,
by choosing a form for the prior which allows the value for a parameter at one
voxel to influence those at its spatial neighbours. An example is to model the prior
as a multi-variate normal distribution over the *voxels*:

.. math::
    P(\Theta_k) = MVN(0, (\phi_k D)^{-1})

.. note::
    It is important to remember that this is an MVN over the *voxels* in the data for
    a single parameter, not a distribution over the different parameters in the model.

Here :math:`\Theta_k` 
is a vector of all the values for parameter :math:`k` at the various voxels, 
:math:`\phi_k` is a global smoothing parameter for model parameter :math:`k` and
:math:`D` is a covariance matrix which reflects the influence of voxels on
their spatial neighbours, 

What this means is that the prior probability of a parameter map is a function of the
entire set of values at all the voxels, rather than simply a set of independent 
probabilities of the values at individual voxels.

:math:`\phi_k` is allowed to vary during the optimization. A high value of :math:`\phi_k`
corresponds high prior spatial smoothness while a low value of :math:`\phi_k` reduces the
influence of a voxel on its neighbours and corresponds to a less smooth output.

The log PDF (latent cost, in the SVB formalism) can be derived as:

.. math::
    \log{P(\Theta_k)} = \sum_i \frac{1}{2} \log{\phi_k} - \frac{\phi_k}{2} \Theta_{ki} (D\Theta_k)_i

Where the sum is over the voxels :math:`i`. This decomposes into two terms, one favouring
high :math:`\phi_k` (increased smoothing), and the other favouring low :math:`\phi_k`
(less smoothing). The latter term is connected to the actual spatial variability of the
parameter. Hence the prior adapts to the variation in the data to infer an optimal 
value of :math:`\phi_k` and a parameter map which is smoothed accordingly.

Since :math:`\phi_k` is now being inferred itself, it also requires a prior. Following
Penny, we use a relatively non-informative Gamma distribution:

.. math::
    P(\phi_k) = Ga(\phi_k; 10, 1)

:math:`D` is an :math`N \times N` matrix where :math:`N` is the number of voxels. It can be chosen 
in various ways. Currently we use a choice of :math:`D`
derived from a Markov Random Field model, in which the diagonal elements of :math:`D` are
the number of nearest spatial neighbours for the corresponding voxel while the off-diagonal
elements are -1 for voxels which are nearest neighbours. It can be shown that this is 
equivalent to a prior PDF of the form:

.. math::
    P(\Theta_k) \propto \phi_k^{\frac{N}{2}} \exp(-\frac{\phi_k}{4}\sum_i \sum_{j \in N(i)} (\Theta_{ki} - \Theta_{kj})^2)

Where :math:`N(i)` is the set of neighbouring voxels to voxel :math:`i`. Hence this 
prior penalizes differences between neighbouring voxels.

Alternative choices of :math:`D` are possible, for example Penny et al (2004) uses a
Laplacian operator to derive a :math:`D` which includes influence from second nearest
neighbours in addition.

For practical calculation, it is important that the matrix :math:`D` is sparse - with 
typically :math:`10^5` to :math:`10^6` voxels it is difficult to store the full matrix 
within memory. Calculation of quantities involving :math:`D` are typically done either
by looping over voxelwise nearest neighbour lists, or by representing :math:`D` within
a dedicated sparse matrix library.

Implementation of spatial priors in analytic Variational Bayes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To implement this prior in the analytic VB framework we must derive a new set of 
update equations for the spatial smoothing parameter :math:`\phi_k`, which 
depends on the current posterior parameter estimates. Typically we update the
posterior parameters for all voxels with a fixed :math:`\phi_k` and then update
:math:`\phi_k` itself before repeating the voxelwise update loop.

Implementation of spatial priors in stochastic Variational Bayes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is more straightforward than the analytic case, because we simply need to 
use the expression given above for the log PDF of the spatial prior within the
calculation of the latent cost. :math:`\phi_k` is included as a parameter of 
optimization and is automatically updated to minimise the total cost.

Either of the expressions for the log PDF given above can be used, however in 
practice the version expressed in terms of (sparse) matrix multiplication is
easier to implement in an efficient way within the TensorFlow library. Both, 
however, produce the same results when tested.
